---
title: "The 5-Minute Code Review: How I Evaluate AI-Generated Code Without Getting Burned"
subtitle: "A mental model for quickly spotting problems in AI output"
summary: "When AI generates code, you need to review it fast—but not so fast you miss critical issues. Here's my checklist for evaluating AI output in minutes, not hours."
image: "/images/gallery/horizontal-3.jpg"
publishedAt: "2026-02-07"
tag: "AI Engineering"
---

## The Bug That Changed How I Review

The function looked fine:

```python
def process_payment(amount, currency):
    if currency == "USD":
        return stripe.charge(amount)
    return None
```

I reviewed it, approved it, shipped it. What I didn't catch: the function returns `None` for every non-USD currency with no exception, no error code, no signal that anything went wrong. The calling code upstream assumed failure would raise an exception — so when a Canadian user tried to pay in CAD, the payment silently evaporated. No charge, no error message, no log entry. Just nothing.

We caught it three days later during a billing reconciliation. The payment processor had no record of the transactions. I had to trace back through logs to figure out what had happened and why, then explain it to a client who had been wondering why their Canadian signups weren't converting.

I had reviewed that code. I checked the security, I checked the SQL queries, I spot-checked the happy path. What I had not done was ask: *what happens when this function takes a code path that isn't the happy path?* The answer, in this case, was "nothing at all."

That's the bug type AI is most reliably bad at: not broken logic, but absent logic. The unhandled case. The path that exists and isn't covered. The function that succeeds at what it's supposed to do and silently fails at what it's supposed to prevent.

Since then, my review process has been built around that failure mode first.

---

## How to Actually Review AI Code in Five Minutes

The five minutes is real, but it requires pattern recognition — knowing where AI fails so you can check those spots directly instead of reading line by line. Here's the order that matters:

### Error Handling First

This is where AI consistently falls short. It implements the happy path with confidence and handles failure as an afterthought, often by not handling it at all. What I check:

- What does this return when the external call fails? When the database is slow? When the input is empty?
- If this function returns `None`, is the calling code prepared for that?
- Are exceptions raised explicitly, or does the function silently swallow them?
- What does the error message say when something goes wrong — will I be able to diagnose this in production, or will I be staring at "something went wrong"?

The payment bug above would have died here. "What does this return for non-USD currency?" is a five-second question.

### Security Issues

I look at this second because the failure mode is catastrophic rather than silent. The patterns I check immediately:

- User input used directly in database queries (the classic SQL injection)
- Endpoints with no authentication check — AI generates authenticated and unauthenticated versions with similar structure; it's easy to miss a missing `@require_auth` decorator
- Secrets hardcoded in the function (API keys, connection strings that should be env vars)
- Insecure defaults — permissive CORS, weak encryption parameters, logging that captures sensitive data

AI misses these not because it doesn't know they exist but because it doesn't know your threat model. It generates code that's technically correct for an idealized system. Your system has actual users doing actual adversarial things.

### Performance Red Flags

Here I'm scanning for the structural problems that don't show up until load:

- Loops that make database calls — the N+1 problem. Any `for` loop that contains a database query inside a request handler should be a red flag until I understand why it's there.
- No upper bounds on collection operations — if this can return 10 records or 100,000 records and the code doesn't distinguish between those cases, you have a latency bomb.
- Synchronous operations that should be async, especially for anything that hits a network or the filesystem.

AI doesn't think about scale. It writes code that works for the test case and doesn't think about what happens at ten thousand concurrent requests.

### Integration Points

This is where context matters most and AI is most likely to guess wrong:

- Does the function signature match what the calling code expects? Parameter names, order, return type?
- Is the AI following existing patterns in the codebase, or has it invented its own?
- Will this code break anything that depends on shared state?

This is the "generated code against an imaginary codebase" problem I've written about before. Without explicit context, AI infers your conventions — and its inference is sometimes wrong.

### Readability

Last, and genuinely last — will this code be readable by a developer who didn't write it? I'm not checking style preferences. I'm asking whether someone debugging this at 2am will be able to understand it quickly enough to fix it.

---

## The Catches and the Misses

**Caught in seconds:**
```python
def get_user_data(user_id):
    query = f"SELECT * FROM users WHERE id = {user_id}"
    return db.execute(query)
```

SQL injection via f-string interpolation. This gets caught by the security check before I even get to the rest of the function.

**Caught in a minute:**
```javascript
async function fetchUserPosts(userId) {
    const user = await getUser(userId);
    const posts = [];
    for (const postId of user.postIds) {
        posts.push(await getPost(postId));
    }
    return posts;
}
```

N+1 query inside a request handler. If a user has 200 posts, this makes 201 sequential database calls. Found by the performance scan.

**Missed initially, caught in testing:**
A data import function that worked correctly when the input file was well-formed but hit an unhandled exception when a required column was missing — not caught because my test data was clean. Error handling check is first on the list now specifically because of patterns like this.

---

## When Five Minutes Isn't Enough

Some code warrants a full review regardless of time pressure:

- Authentication and authorization logic — a mistake here is a breach
- Payment processing — see above
- Code that touches multiple systems or exports a shared interface
- Anything using an algorithm or pattern I haven't seen before

For these, I read every line, trace the execution paths through the edge cases, and write tests specifically designed to break the implementation before I trust it. The five-minute review is for routine implementation. There's no shortcut for the critical path.

---

*The silent failure bug — function returns None instead of raising — is the one I see most often in AI-generated code and least often caught in code review. What's the category of mistake you find yourself checking for first?*
