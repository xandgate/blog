---
title: "The Architecture Decisions AI Can't Make For You"
subtitle: "High-stakes choices that still require human judgment"
summary: "AI can implement almost anything, but some decisions require context, judgment, and experience that AI simply doesn't have. Here are the architecture choices I still make myself."
image: "/images/gallery/vertical-1.jpg"
publishedAt: "2026-02-21"
tag: "AI Engineering"
---

## The Time I Let AI Make an Architecture Decision

I want to start with the failure, because the failure is the point.

Two years ago I was designing the data layer for a healthcare benefits platform — a system that would eventually sit between Cigna's member records and a consumer-facing portal. I was wrestling with the caching strategy. The access patterns were read-heavy, the data changed infrequently, latency mattered. I described the problem to Claude in detail: read/write ratios, data size estimates, update frequency, the tech stack we were on.

Claude recommended Redis with a write-through cache and a TTL of fifteen minutes. The analysis was coherent. The tradeoffs were laid out clearly. I built it.

What Claude didn't know — couldn't know — was that member eligibility data in healthcare can change same-day due to qualifying life events. Someone gets married, they're in their open enrollment window, they update their plan, and they need their new benefits confirmed immediately. A fifteen-minute cache means they see stale eligibility data during the most consequential moments of their interaction with the system. This isn't a UX annoyance. In healthcare, it's a compliance exposure.

The AI had reasoned about my technical constraints perfectly. It had no way to reason about the regulatory and business context I hadn't given it — and crucially, I hadn't known to give it, because I hadn't thought to ask myself those questions first. That's the failure mode nobody talks about: AI doesn't just answer what you ask, it answers *only* what you ask. The questions you don't think to raise are the ones that get you.

We caught it in QA before it shipped. But the rebuild cost a week and required a more complex invalidation strategy that made the system harder to maintain. I own that decision. But I made it the wrong way.

---

## System Boundaries Are Organizational Decisions Wearing Technical Clothing

The most expensive architecture mistake I see repeatedly — in government projects, in enterprise platforms, in healthcare systems — is drawing service boundaries around technical criteria instead of organizational ones.

On a federal workforce development project, I inherited a codebase that had been split into eleven microservices by an architect who had clearly read the right books. Each service was technically coherent. The problem was that five of those services were owned by two teams that couldn't agree on a deployment cadence, which meant any feature that touched both took three weeks to ship because it required synchronized releases across two organizational fiefdoms.

Conway's Law is not a suggestion. Systems are shadows of the organizations that build them. The question "where should this service boundary be?" is really "which team will own this?" and "what happens when that team changes?" AI can give you a thorough technical analysis of coupling and cohesion. It cannot tell you that the team that would own Service B has two people, both of whom are interviewing elsewhere.

My process for this now: before I draw any system boundary, I draw the org chart first. The technical architecture should make the organizational communication load lighter, not heavier. If a boundary requires intensive coordination across teams to function, the boundary is wrong, regardless of how clean it looks on a diagram.

AI is genuinely useful in this conversation — give it the org chart and the communication patterns alongside the technical requirements and it can help you stress-test options. But you have to give it everything, including the things that aren't in the Confluence doc. The political constraints, the team capability gaps, the things everyone knows but nobody has written down. Without that context, its recommendations will be technically correct and organizationally naïve.

---

## Failure Modes Are a Business Judgment, Not a Technical One

The last architectural category where I've consistently found AI insufficient is failure resilience — not implementing it, but deciding what to protect against and how hard.

AI will, by default, recommend you protect against everything. Retry logic, circuit breakers, dead letter queues, idempotency keys. I've received AI-generated architecture proposals that were technically bulletproof and would have taken eighteen months to build. Bulletproof is not the goal. Appropriate resilience for the actual failure scenarios that matter is the goal.

On a government of Jamaica digital services project, we had to design a system that processed benefit applications and sent notifications via SMS. The SMS vendor had a 99.5% uptime guarantee — fine, but that 0.5% downtime concentrated unpredictably. The question wasn't "how do we implement a retry mechanism?" It was "what happens to an applicant who doesn't get a confirmation SMS because of a two-hour outage?"

The answer, once we traced it through the actual process, was: they show up at a physical office to confirm their application status. That's bad, but it's recoverable. It's not data loss. It's not a compliance failure. It's an inconvenience with a known fallback.

That context changed the resilience architecture entirely. We built a simple retry queue with a 24-hour window and a fallback email notification. We did not build a distributed transaction log or a multi-vendor SMS failover. The simpler system was correct for the actual stakes.

AI will ask what your resilience requirements are. It will not ask whether you've actually traced the business impact of each failure mode through to its real-world consequence. That's the question that determines whether you're building appropriate protection or elaborate infrastructure theater.

Architecture is where experience earns its money. Not because AI lacks intelligence, but because the inputs it needs to make good recommendations are the things hardest to articulate: what actually matters to the business, how the organization actually operates, what the people in this system actually do when things go wrong. Those things live in your head, built up from years of watching systems fail and watching people respond. Until you learn to put them in the prompt, the best AI in the world is working from an incomplete picture.

---

*What's the architectural decision you've most regretted delegating — to AI, to a vendor, or to a consultant who didn't know your context? I'd be curious what the missing context was.*
