---
title: "Building a Full-Stack Feature in 2 Hours: A Real-Time Walkthrough"
subtitle: "End-to-end demo of AI-augmented development"
summary: "Watch me build a complete feature from database schema to frontend UI in 2 hours using AI. This is what modern development looks like when you leverage AI effectively."
image: "/images/gallery/vertical-2.jpg"
publishedAt: "2026-02-28"
tag: "AI Engineering"
---

## What Actually Happened

I want to be honest with you: most "I built X in Y hours with AI" posts are laundered fiction. The author compresses three days of false starts into a clean two-hour narrative, strips out every wrong turn, and presents a tutorial where real development used to be. I've read dozens of them. I've written one. You're looking at the replacement.

This is a real account of building a "saved articles" feature for a blog platform — users can bookmark posts, view their list, remove items. Full stack: database migration, API endpoints, frontend components, state management. I ran a timer. I kept notes. The parts where AI failed are in here too, because that's where the actual learning lives.

The stack: FastAPI on the backend, Svelte on the frontend, PostgreSQL, Alembic for migrations. AI tools: Cursor with Claude for implementation, Claude directly when I needed to think through architecture. Existing codebase with auth already wired up.

Final time: 2 hours 38 minutes. Not two hours. Here's where the extra 38 went.

---

## Hour One: Backend

### The Migration (Expected: 10 min / Actual: 10 min)

This one actually went clean. I gave Cursor a specific prompt:

> *"Create an Alembic migration for a favorites table. Columns: id (PK), user_id (FK to users), article_id (FK to articles), created_at. Add a unique constraint on user_id + article_id. Add an index on user_id for query performance."*

It generated exactly what I asked for:

```python
# migrations/003_add_favorites.py
from alembic import op
import sqlalchemy as sa

def upgrade():
    op.create_table(
        'favorites',
        sa.Column('id', sa.Integer(), nullable=False),
        sa.Column('user_id', sa.Integer(), nullable=False),
        sa.Column('article_id', sa.Integer(), nullable=False),
        sa.Column('created_at', sa.DateTime(), server_default=sa.func.now(), nullable=False),
        sa.PrimaryKeyConstraint('id'),
        sa.UniqueConstraint('user_id', 'article_id', name='uq_user_article'),
        sa.ForeignKeyConstraint(['user_id'], ['users.id'], ondelete='CASCADE'),
        sa.ForeignKeyConstraint(['article_id'], ['articles.id'], ondelete='CASCADE'),
    )
    op.create_index('ix_favorites_user_id', 'favorites', ['user_id'])

def downgrade():
    op.drop_table('favorites')
```

The `ondelete='CASCADE'` was a nice touch I hadn't specified — if a user or article is deleted, their favorites go too. I'd have added it anyway but I didn't have to ask. This is AI at its best: filling in the obvious implication you didn't think to say explicitly.

Migration ran clean. Moved on.

### The API Endpoints (Expected: 30 min / Actual: 55 min)

This is where the session got interesting.

My prompt:

> *"Create three FastAPI endpoints: POST /api/favorites to add an article to the current user's favorites, GET /api/favorites to return the user's list with article metadata, DELETE /api/favorites/{article_id} to remove. Use the existing auth dependency pattern from the codebase. Include error handling for duplicate favorites and non-existent articles."*

The first generation looked right at a glance. Clean structure, proper HTTP status codes, used the `get_current_user` dependency from auth. I almost moved on.

I didn't, because I noticed something: the GET endpoint returned a list of `favorite` records with just `article_id`. No article title, no URL, no metadata. Completely useless for a frontend that needs to display a list of saved articles. The prompt said "article metadata" and the AI technically produced code that queries the favorites table — it just didn't join to the articles table.

So I refined:

> *"The GET /api/favorites endpoint should return full article details, not just IDs. Join favorites with articles and return title, slug, published_at, and summary for each."*

Second generation fixed that, but introduced a new problem: it used a raw SQL join instead of SQLAlchemy's ORM relationship. The rest of the codebase uses the ORM. I could have left it — it would have worked — but inconsistent patterns are how codebases turn into archaeology projects six months later.

Third prompt:

> *"Rewrite using SQLAlchemy ORM with a relationship between Favorite and Article models. The existing Article model is in models/article.py."*

I had to give it the file path explicitly. Without that, it invented a schema that didn't match ours. This is a pattern I've hit repeatedly: AI generates confident, syntactically correct code against a model of your codebase that doesn't exist. The code is right — just not for your project.

Third generation was correct. I added one thing by hand: pagination on the GET endpoint. AI had skipped it. Not wrong exactly, but a favorites list with no limit is a ticking time bomb for users who've saved three hundred articles.

Total API time: 55 minutes, not 30.

### Tests (Expected: 20 min / Actual: 20 min)

Prompt:

> *"Write pytest tests for the three favorites endpoints. Test: adding an article, retrieving favorites (verify article metadata is included), removing a favorite, attempting to add a duplicate (expect 409), attempting to remove a non-existent favorite (expect 404), and accessing the endpoints without authentication (expect 401)."*

The tests generated cleanly and passed on first run. One observation: when I gave AI a specific list of test cases, it nailed it. When I've said "write comprehensive tests" without specifics, I get tests that check status codes but nothing meaningful about the response body. The specificity of the prompt determines the quality of the output.

**Backend total: 1 hour 25 minutes.**

---

## Hour Two: Frontend

### API Client (Expected: 15 min / Actual: 15 min)

Prompt:

> *"Create TypeScript-style JSDoc-annotated Svelte functions: addFavorite(articleId), getFavorites(), removeFavorite(articleId). Use fetch, handle errors by throwing with a message from the response body, return parsed JSON on success."*

Clean output. I added loading state management by hand — three `let loading = false` guards around each async call. Not because AI couldn't have done it, but because I wanted to think through the UX myself. When should the button be disabled? Only during that specific action, or whenever any favorite operation is in flight? That's a product decision, not a code problem.

### The UI Components (Expected: 30 min / Actual: 47 min)

This is the part I've thought about most.

The prompt:

> *"Create a Svelte component FavoriteButton.svelte that shows a bookmark icon, toggling filled/outlined based on whether the current article is favorited. On click, call addFavorite or removeFavorite. Show a loading spinner during the operation. Match the design system — use the existing Button component and the lucide-svelte icon library."*

First output: completely ignored the existing Button component, imported its own styling, used heroicons instead of lucide-svelte. Visually similar but architecturally wrong — now I have two icon libraries in a project that had one.

The problem was scope. I said "match the design system" without giving it the design system. AI cannot read files it hasn't been shown.

I restructured: opened `FavoriteButton.svelte`, dragged it into the Cursor context pane, then added the existing `Button.svelte` and the imports from another component I knew used lucide-svelte correctly. Regenerated.

Second output was right. Icon, toggle logic, loading state, accessibility attributes (aria-label, aria-pressed). It even added a subtle scale animation on the icon toggle that I hadn't asked for and kept because it was good.

The favorites list page was similar — one generation that misunderstood the routing structure, a context-enriched second that got it right, plus I wrote the empty state text by hand because AI's placeholder text ("No favorites yet. Start saving articles!") was too cheerful for my taste.

**Frontend total: 1 hour 13 minutes.**

---

## Where the Time Actually Went

A breakdown that reflects reality:

| Task | Expected | Actual | Delta |
|------|----------|--------|-------|
| Database migration | 10 min | 10 min | — |
| API endpoints | 30 min | 55 min | +25 min (join logic, ORM refactor) |
| Backend tests | 20 min | 20 min | — |
| Frontend API client | 15 min | 15 min | — |
| UI components | 30 min | 47 min | +17 min (context + icon library) |
| Integration + manual QA | 15 min | 11 min | -4 min |
| **Total** | **2:00** | **2:38** | **+38 min** |

The 38 extra minutes came from two places: AI generating code against an imagined version of my codebase, and me not front-loading enough context to prevent it. Both are correctable.

The lesson isn't "AI is slower than advertised." It's that AI's accuracy is a direct function of the context you give it. The migration took 10 minutes because my prompt was airtight. The API endpoints took nearly an hour because I assumed "use the existing pattern" was sufficient instruction.

## What Didn't Change

I still made every architectural decision. Should favorites cascade on user delete? Yes — orphaned records create data integrity problems. Should I paginate the list? Yes — always paginate collections. Should the button be disabled during load? Yes — prevent double-submits. None of that came from AI. It came from having built enough production systems to know what breaks.

What changed is that I didn't have to write the code to implement those decisions. I made the call, AI wrote the implementation, I reviewed and adjusted. That's a fundamentally different way to work, and it took me a few months to stop feeling guilty about it.

## The Real Benchmark

The question isn't whether I built this in two hours or two hours and thirty-eight minutes. It's whether I'd spend a full day on this without AI. I would have. The database migration alone — looking up Alembic syntax, getting the constraint naming right, verifying the index creation — would have burned thirty minutes. Multiply that friction across every piece of code and you get a day.

AI collapsed that friction. Not to zero, but to something manageable. The overhead shifted from "remembering syntax" to "providing context," and that's a trade I'll take every time.

---

*If you've hit the "AI generated wrong-codebase code" problem I described above, I'd be curious how you solved it. Do you front-load context at session start, or build it prompt by prompt? That's the workflow question I'm still refining.*
