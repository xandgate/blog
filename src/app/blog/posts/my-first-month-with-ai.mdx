---
title: "My First Month with AI: Excitement, Frustration, and Finding My Flow"
subtitle: "Honest account of the learning curve"
summary: "The first month of using AI coding assistants was a rollercoaster. Here's what I learned, what surprised me, and how I found my rhythm."
image: "/images/gallery/vertical-4.jpg"
publishedAt: "2026-03-14"
tag: "AI Engineering"
---

I installed Cursor on a Monday in January. By that Friday I had shipped more code than in the previous two weeks combined. By the following Wednesday I was staring at a production bug I couldn't explain, wondering if I had fundamentally broken my ability to write software.

That arc — from astonishment to terror to something that finally feels like competence — is the story of my first month. I want to tell it honestly, which means telling you about the Wednesday.

---

The first few days were genuinely intoxicating. I asked Cursor to implement a complex intake form I'd been avoiding: twelve fields, conditional validation logic, an API integration that needed to handle partial failures gracefully. The kind of task I'd normally break into a half-day project, plan out on paper, then execute carefully.

The AI did it in forty seconds.

I reviewed the code, made a handful of tweaks, ran it through QA, shipped it. The whole thing took maybe an hour. I remember sitting back in my chair with the particular feeling of having cheated on a test and gotten away with it — which, I now recognize, was a warning sign I should have paid more attention to.

The next ten days were like that. Feature after feature, faster than I'd ever moved. I started picking up tickets I'd deprioritized for weeks. I stopped blocking on unfamiliar APIs because I could just ask. I felt, genuinely, like a different kind of engineer.

What I was not doing was understanding the code I was shipping.

---

That brings me to the Wednesday.

I was working on a data export feature for a government client — nothing exotic, just pulling records and formatting them as CSV. The AI wrote the function in a minute. I reviewed it, it looked fine, it passed my tests, I shipped it.

Three days later, a client email: some of the exported files were corrupted. Not all of them. Just the ones with records containing commas in text fields.

I pulled up the code. The CSV serialization had no quoting logic. If a description field contained a comma, the output would split that field across two columns, shifting every subsequent column offset by one. The function had passed my tests because my test data was clean strings. The AI had written a CSV serializer that didn't handle the most fundamental CSV edge case.

Here's what unnerved me: I had *read* that code. I looked at it before shipping. But I read it the way you read something you expect to be right — skimming for structure, not interrogating for failure modes. The AI's code was confident and clean, and confidence reads as correctness when you're moving fast.

I spent four hours on that bug, including an hour just understanding what had gone wrong well enough to explain it to the client. That hour was the worst part. I'm used to owning my bugs. This one felt borrowed — someone else had written it, I'd nodded it through, and now I was accountable for it.

I almost stopped using the AI tools entirely that afternoon. Not out of anger at the tools — rationally I knew the bug was mine to catch — but because I didn't trust my own judgment anymore. If I had missed that, what else was I missing? I had shipped ten features in two weeks. How many of them had quiet landmines I hadn't triggered yet?

---

I didn't stop. But I changed how I worked.

The shift was conceptual before it was practical: I stopped thinking of AI as a code generator and started thinking of it as a first draft. A first draft is useful. A first draft is not finished. The value isn't that you don't have to write — it's that you start with something to react to instead of a blank page.

With that reframe, my review process got sharper. I started interrogating generated code the way I'd interrogate a junior developer's PR: not just "does this look right" but "what are the inputs this breaks on, what does it return when the external call fails, what happens at scale." The CSV bug would have died there.

I also started being more deliberate about what I handed off. Routine implementation — database queries, API wrappers, serialization logic — AI drafts first, I review carefully. Ambiguous problems where I'm not sure what I want yet — I think first, use AI to pressure-test, implement myself. Architecture — mine entirely.

The speed advantage dropped when I got rigorous. But the code quality came back up, and more importantly, I trusted what I was shipping again. That trust is not a soft metric. It affects how you work, how fast you move, and frankly how you feel at the end of the day.

---

By the end of the month I was better at this than I had been at the start, which is obvious. Less obviously: I was also better at writing software than I had been before I started. Not because the AI taught me anything explicitly — but because the volume of code I was reviewing forced me to articulate, clearly and quickly, why a piece of code was right or wrong. That kind of rapid-fire code review is the fastest path to sharper judgment I've found in fifteen years.

The engineers who get the most from these tools, I've come to believe, are not the ones who prompt best. They're the ones who can look at a hundred lines of generated code and know, within two minutes, whether it's sound. That's a skill built over years of writing, debugging, and shipping real systems. AI didn't obsolete it. It made it the limiting factor.

---

*A question I think about: at what point does heavy AI use start to atrophy the skills that make you good at reviewing AI output? I don't have a clean answer. But if you're early in your career and leaning heavily on these tools, it's worth sitting with.*
