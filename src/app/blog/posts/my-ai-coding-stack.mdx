---
title: "My AI Coding Stack: Claude, Cursor, and the Art of Knowing When to Intervene"
subtitle: "The tools I actually use, the prompts that work, and when to take the wheel"
summary: "After months of experimenting with AI coding assistants, here's my actual workflow with Claude and Cursor—including real prompts that consistently produce good results."
image: "/images/gallery/horizontal-2.jpg"
publishedAt: "2026-01-31"
tag: "AI Engineering"
---

Most "AI tools I use" posts are really equipment catalogs: here's the thing, here's the logo, here's a vague endorsement. I want this to be more useful than that. What follows is my actual stack after extended daily use — the specific prompts that work, what "working" looks like in practice, what bad output looks like, and why I dropped the tools I dropped.

## The Stack

**Claude** — primary reasoning tool. Architecture discussions, hard debugging, code review, anything that requires context-holding across a long conversation. I use the API directly for extended sessions and via cursor's built-in chat for in-context work.

**Cursor** — the coding environment. Worth the switch from VS Code specifically because it maintains project-wide context without me having to paste it repeatedly. The difference between "explain this error" in a generic chat and "explain this error" in a tool that knows your schema, your patterns, and your last fifty changes is substantial.

**GitHub Copilot** — I dropped this after four months. The reason is specific and, I think, underappreciated: Copilot completes code based on what you're typing, which means it's completing your intent rather than questioning it. When I'm moving fast and slightly wrong, Copilot helpfully extends my wrongness. Claude and Cursor in chat mode force me to articulate what I want, and that articulation catches mistakes before they're in the code. Autocomplete optimizes for the experience of not being interrupted. That experience is overrated.

That's it. Two tools. Every additional tool is overhead until you've built deep fluency with the first two.

---

## The Prompts, With Real Examples

Prompt engineering advice usually reads like horoscopes: general enough to feel true, specific enough to feel actionable, too vague to actually implement. Here's what I use, with examples of what good output and bad output actually look like.

### Architecture Decision Prompt

```
I'm building [feature] that needs to [core requirement]. 
The system currently [current architecture in 2-3 sentences].
I'm deciding between [option A] and [option B].

Hard constraints:
- Must integrate with [specific existing system]
- Performance floor: [specific metric]  
- Team: [size + relevant skill gaps]
- Timeline: [weeks]

Walk me through the tradeoffs. Give me a recommendation with explicit reasoning about which constraints drove it.
```

**What good output looks like:** Claude picks a recommendation and defends it. It cites the specific constraints that made the difference — "given your two-person team and six-week timeline, the operational complexity of option B is the deciding factor, not the technical merits." It surfaces a risk you hadn't mentioned. It asks one clarifying question if there's genuine ambiguity.

**What bad output looks like:** A symmetric list of pros and cons for both options with "ultimately it depends on your priorities" at the end. If you get this, your prompt is missing constraints. Add more constraints. The less you leave underdetermined, the more useful the output.

**Real example:** I used this prompt designing a caching layer for a healthcare portal. I described the read/write patterns, the team size, the compliance sensitivity of the data. Good output: it recommended against Redis with a long TTL for reasons it explained clearly (member eligibility changes same-day for qualifying life events). It also flagged a compliance angle I hadn't raised. That's the signal that you've given it enough context — it's surfacing things you didn't think to mention.

---

### Debugging Prompt

```
I'm seeing [specific error or behavior].

Relevant code:
[snippet]

Expected: [what should happen]
Actual: [what's happening]
Already checked: [list of hypotheses you've ruled out]

What's the most likely cause? Give me a ranked list of 3 hypotheses with a quick way to verify each.
```

**What good output looks like:** Hypothesis 1 is usually right. The verification steps are specific — "add a log statement at line X and check whether the value is null before the conditional" rather than "add some logging." It uses the "already checked" list to not waste your time.

**What bad output looks like:** It suggests the first three things you already ruled out, or it gives you a hypothesis with no clear way to verify it. This almost always means your "already checked" list wasn't specific enough. "I checked the auth" is weak; "I verified the JWT contains the correct user_id by logging it at the point of token validation" is strong.

**The thing most people skip:** The ranked list. "What's wrong?" produces a paragraph. "Three ranked hypotheses with verification steps" produces an actionable debugging sequence. The structure of the ask shapes the structure of the response.

---

### Code Review Prompt

```
Review this code for production readiness. Flag issues in this priority order:
1. Security vulnerabilities (especially auth, data exposure, injection)
2. Failure modes that aren't handled (what happens when X is null, Y service is down)
3. Performance issues at scale (this will handle ~[expected load])
4. Maintainability problems a developer unfamiliar with this code would hit

[code]

Skip style issues. Flag anything you'd block a PR for.
```

**What good output looks like:** It finds the failure mode you didn't test. Specifically, it finds the "what happens when the external call returns 200 but with an unexpected schema" case, or the "what does this return when the input list is empty" case. It cites the specific line. It explains the impact.

**What bad output looks like:** "The code looks clean and well-structured. A few things to consider..." followed by style feedback. This means the code is actually fine and you're using this prompt unnecessarily, OR the prompt didn't have enough context about expected load and environment. Add the load expectation explicitly. AI is optimistic about performance by default; tell it to be pessimistic.

---

### Implementation Prompt (The One People Underuse)

```
Implement [function/component/module]. 

Requirements:
- [specific behavior 1]
- [specific behavior 2]
- [edge case 1]
- [edge case 2]

Context:
- This fits into [existing system description]
- Follow the pattern in [specific file or function you can paste or reference]
- Language: [language], framework: [framework]

Return only the implementation, no explanation unless you had to make a non-obvious decision.
```

**What good output looks like:** Code that follows the existing patterns, handles the edge cases you listed, and is readable without annotation. If it made a non-obvious decision, it explains why in a single sentence.

**What bad output looks like:** Boilerplate that doesn't match your codebase style, missing edge case handling, and a paragraph of explanation for code that should be self-explanatory. The cause is almost always "follow the pattern in [existing code]" — without that anchor, AI generates to its own conventions, not yours.

---

## My Daily Workflow

**Task breakdown (10 min at start of day):** I paste the ticket description into Claude and ask for implementation steps, dependencies, and potential blockers. This catches the "oh, this requires a schema change" issue before I'm two hours into the frontend.

**Active development:** Cursor with chat open. I implement one unit at a time — one function, one component — review immediately, then move on. I don't let AI-generated code accumulate unreviewed.

**Code review (30 min):** I paste larger chunks into Claude using the code review prompt above. This is where the "already checked" logic pays off — I tell it what I already reviewed and it goes deeper on the rest.

**End-of-session extraction (5 min):** "What implementation patterns did I use today that I should document?" This has built a useful personal library of prompts and patterns over several months.

---

## When to Override the AI

The rule I've settled on: AI leads when the task is well-defined and the failure cost is recoverable. I lead when the failure cost is architectural or when the requirements are ambiguous.

Specifically, I take the wheel when:
- The decision will be hard to reverse (schema design, service boundaries)
- The requirements aren't clear — AI implementing an ambiguous requirement makes it concrete in the wrong way
- The code touches a system with hidden dependencies I know about and the AI doesn't
- I'm debugging something AI-generated that's already broken — at that point, AI is going in circles and I need to reason through it myself

The worst AI-assisted code I've written came from using AI to push through ambiguity instead of resolving it first. The output looked fine and was wrong in ways that took weeks to surface.

---

*If you've built a prompt that reliably produces something mine doesn't — particularly for debugging distributed systems or legacy code — I'd genuinely like to see it. That's the prompt space I'm still refining.*
